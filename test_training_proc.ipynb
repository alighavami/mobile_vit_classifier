{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5106717b",
      "metadata": {
        "id": "5106717b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/ali/ws/xaje-nasir/mechatronic/project/python-mech1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import timm\n",
        "import urllib\n",
        "\n",
        "from timm.data import resolve_data_config\n",
        "from timm.data.transforms_factory import create_transform\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from PIL import Image\n",
        "import os\n",
        "from einops import rearrange\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import random\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ZU0VdLgwZc6Y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZU0VdLgwZc6Y",
        "outputId": "4f4504e5-60e7-4274-b2a4-a780ca07f7cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d353f930",
      "metadata": {},
      "outputs": [],
      "source": [
        "vill_doc_dir = \"data/village_doc\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8122f454",
      "metadata": {
        "id": "8122f454"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch: 2.6.0+cu124\n",
            "CUDA available: True\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "# === Edit these for your machine ===\n",
        "DATASET_ROOT = vill_doc_dir   # folder that contains class subfolders or train/test\n",
        "DATA_ROOT    = r\"vill_doc_samp\"  # where to create a small sample split\n",
        "MAKE_SAMPLE  = True        # True => create a small sample split at DATA_ROOT\n",
        "MAX_PER_CLASS = 60         # cap per class when building the sample\n",
        "TRAIN_RATIO   = 0.8        # train/test split when sampling\n",
        "IMG_SIZE      = 224\n",
        "BATCH_SIZE    = 64\n",
        "EPOCHS        = 3\n",
        "NUM_WORKERS   = 4\n",
        "\n",
        "# ------------------ env check ------------------\n",
        "import os, sys, random, shutil\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# Optional: pin determinism lightly (keeps cudnn fast)\n",
        "import numpy as np, random\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED)\n",
        "torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = False\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "DATASET_ROOT = Path(DATASET_ROOT)\n",
        "DATA_ROOT    = Path(DATA_ROOT)\n",
        "\n",
        "# quick sanity\n",
        "if not DATASET_ROOT.exists():\n",
        "    raise FileNotFoundError(f\"DATASET_ROOT not found: {DATASET_ROOT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "236a245e",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b47db146",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "e2ec82ba",
      "metadata": {},
      "source": [
        "Build (optional) small sample split with train/ & test/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e5cafba8",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def has_split(root: Path) -> bool:\n",
        "    return (root / \"train\").is_dir() and (root / \"test\").is_dir()\n",
        "\n",
        "def guess_class_roots(root: Path):\n",
        "    # if already split, return train dir classes\n",
        "    if has_split(root):\n",
        "        return sorted([p for p in (root/\"train\").iterdir() if p.is_dir()])\n",
        "    # otherwise, use immediate subfolders as classes\n",
        "    return sorted([p for p in root.iterdir() if p.is_dir()])\n",
        "\n",
        "def build_sample_split(src_root: Path, dst_root: Path, max_per_class=60, train_ratio=0.8, exts={\".jpg\",\".jpeg\",\".png\"}):\n",
        "    if dst_root.exists():\n",
        "        print(f\"[info] removing existing {dst_root} to rebuild sample...\")\n",
        "        shutil.rmtree(dst_root)\n",
        "    (dst_root / \"train\").mkdir(parents=True, exist_ok=True)\n",
        "    (dst_root / \"test\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if has_split(src_root):\n",
        "        # sample from existing split to keep class distribution\n",
        "        for split in [\"train\", \"test\"]:\n",
        "            for cls_dir in sorted((src_root/split).iterdir()):\n",
        "                if not cls_dir.is_dir(): continue\n",
        "                imgs = [p for p in cls_dir.rglob(\"*\") if p.suffix.lower() in exts]\n",
        "                random.shuffle(imgs)\n",
        "                imgs = imgs[:max_per_class]\n",
        "                out_dir = dst_root / split / cls_dir.name\n",
        "                out_dir.mkdir(parents=True, exist_ok=True)\n",
        "                for p in imgs:\n",
        "                    shutil.copy2(p, out_dir / p.name)\n",
        "    else:\n",
        "        # single-level classes in src_root\n",
        "        for cls_dir in sorted(src_root.iterdir()):\n",
        "            if not cls_dir.is_dir(): continue\n",
        "            imgs = [p for p in cls_dir.rglob(\"*\") if p.suffix.lower() in exts]\n",
        "            if not imgs: continue\n",
        "            random.shuffle(imgs)\n",
        "            imgs = imgs[:max_per_class]\n",
        "            n_train = max(1, int(len(imgs) * train_ratio))\n",
        "            train_imgs, test_imgs = imgs[:n_train], imgs[n_train:]\n",
        "\n",
        "            for split, lst in [(\"train\", train_imgs), (\"test\", test_imgs)]:\n",
        "                out_dir = dst_root / split / cls_dir.name\n",
        "                out_dir.mkdir(parents=True, exist_ok=True)\n",
        "                for p in lst:\n",
        "                    shutil.copy2(p, out_dir / p.name)\n",
        "\n",
        "    print(\"[done] sample split at:\", dst_root)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "fdcab09d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[done] sample split at: vill_doc_samp\n",
            "Using data root: data/village_doc\n",
            "Has split (train/test): True\n"
          ]
        }
      ],
      "source": [
        "# choose which root we'll actually train on\n",
        "if MAKE_SAMPLE:\n",
        "    build_sample_split(DATASET_ROOT, DATA_ROOT, max_per_class=MAX_PER_CLASS, train_ratio=TRAIN_RATIO)\n",
        "    TRAIN_ROOT = DATA_ROOT\n",
        "else:\n",
        "    TRAIN_ROOT = DATASET_ROOT\n",
        "TRAIN_ROOT = DATASET_ROOT\n",
        "print(\"Using data root:\", TRAIN_ROOT)\n",
        "print(\"Has split (train/test):\", has_split(TRAIN_ROOT))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "00287c70",
      "metadata": {},
      "outputs": [],
      "source": [
        "class_names = [\n",
        "    \"Apple___healthy\",\n",
        "    \"Apple___Cedar_apple_rust\",\n",
        "    \"Apple___Apple_scab\",\n",
        "    \"Pepper,_bell___healthy\",\n",
        "    \"Pepper,_bell___Bacterial_spot\",\n",
        "    \"Blueberry___healthy\",\n",
        "    \"Cherry_(including_sour)___healthy\",\n",
        "    \"Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot\",\n",
        "    \"Corn_(maize)___Northern_Leaf_Blight\",\n",
        "    \"Corn_(maize)___Common_rust_\",\n",
        "    \"Grape___healthy\",\n",
        "    \"Grape___Black_rot\",\n",
        "    \"Peach___healthy\",\n",
        "    \"Potato___Early_blight\",\n",
        "    \"Potato___Late_blight\",\n",
        "    \"Raspberry___healthy\",\n",
        "    \"Soybean___healthy\",\n",
        "    \"Squash___Powdery_mildew\",\n",
        "    \"Strawberry___healthy\",\n",
        "    \"Tomato___Early_blight\",\n",
        "    \"Tomato___healthy\",\n",
        "    \"Tomato___Bacterial_spot\",\n",
        "    \"Tomato___Late_blight\",\n",
        "    \"Tomato___Tomato_mosaic_virus\",\n",
        "    \"Tomato___Tomato_Yellow_Leaf_Curl_Virus\",\n",
        "    \"Tomato___Leaf_Mold\",\n",
        "    \"Tomato___Septoria_leaf_spot\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c739a711",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Params (M): 1.026123\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "from einops import rearrange\n",
        "\n",
        "def conv_1x1_bn(inp, oup):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.SiLU()\n",
        "    )\n",
        "\n",
        "def conv_nxn_bn(inp, oup, kernel_size=3, stride=1):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(inp, oup, kernel_size, stride, kernel_size//2, bias=False),\n",
        "        nn.BatchNorm2d(oup),\n",
        "        nn.SiLU()\n",
        "    )\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim); self.fn = fn\n",
        "    def forward(self, x, **kwargs):\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim), nn.SiLU(), nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim), nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=32, dropout=0.):\n",
        "        super().__init__()\n",
        "        inner = heads * dim_head\n",
        "        self.scale = dim_head ** -0.5; self.heads = heads\n",
        "        self.to_qkv = nn.Linear(dim, inner*3, bias=False)\n",
        "        self.attend = nn.Softmax(dim=-1)\n",
        "        self.proj   = nn.Sequential(nn.Linear(inner, dim), nn.Dropout(dropout))\n",
        "    def forward(self, x):\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> b h n d\", h=self.heads), qkv)\n",
        "        dots = (q @ k.transpose(-1,-2)) * self.scale\n",
        "        attn = self.attend(dots)\n",
        "        out  = attn @ v\n",
        "        out  = rearrange(out, \"b h n d -> b n (h d)\")\n",
        "        return self.proj(out)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.ModuleList([PreNorm(dim, Attention(dim, heads, dim_head, dropout)),\n",
        "                           PreNorm(dim, FeedForward(dim, mlp_dim, dropout))])\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "    def forward(self, x):\n",
        "        for attn, ff in self.layers:\n",
        "            x = x + attn(x); x = x + ff(x)\n",
        "        return x\n",
        "\n",
        "class MV2Block(nn.Module):\n",
        "    def __init__(self, inp, oup, stride=1, expansion=4):\n",
        "        super().__init__()\n",
        "        hidden = inp * expansion\n",
        "        self.use_res = (stride==1 and inp==oup)\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(inp, hidden, 1, bias=False), nn.BatchNorm2d(hidden), nn.SiLU(),\n",
        "            nn.Conv2d(hidden, hidden, 3, stride, 1, groups=hidden, bias=False), nn.BatchNorm2d(hidden), nn.SiLU(),\n",
        "            nn.Conv2d(hidden, oup, 1, bias=False), nn.BatchNorm2d(oup)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x); return x + out if self.use_res else out\n",
        "\n",
        "class MobileViTBlock(nn.Module):\n",
        "    def __init__(self, dim, depth, channel, kernel_size, mlp_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.conv_local = conv_nxn_bn(channel, channel, kernel_size)\n",
        "        self.conv_proj  = conv_1x1_bn(channel, dim)\n",
        "        self.transformer = Transformer(dim, depth, heads=4, dim_head=max(16, dim//4), mlp_dim=mlp_dim, dropout=dropout)\n",
        "        self.conv_expand = conv_1x1_bn(dim, channel)\n",
        "        self.conv_fuse   = conv_nxn_bn(channel*2, channel, kernel_size)\n",
        "    def forward(self, x):\n",
        "        res = x\n",
        "        x = self.conv_local(x)\n",
        "        x = self.conv_proj(x)              # [B, D, H, W]\n",
        "        B, D, H, W = x.shape\n",
        "        tokens = rearrange(x, \"b d h w -> b (h w) d\")\n",
        "        tokens = self.transformer(tokens)\n",
        "        x = rearrange(tokens, \"b (h w) d -> b d h w\", h=H, w=W)\n",
        "        x = self.conv_expand(x)\n",
        "        x = torch.cat([x, res], dim=1)\n",
        "        return self.conv_fuse(x)\n",
        "\n",
        "class MobileViT(nn.Module):\n",
        "    def __init__(self, image_size, dims, channels, num_classes,\n",
        "                 expansion=4, kernel_size=3, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.stem = conv_nxn_bn(3, channels[0], stride=2)\n",
        "        self.mv2 = nn.ModuleList([\n",
        "            MV2Block(channels[0], channels[1], 1, expansion),  # 16->16\n",
        "            MV2Block(channels[1], channels[2], 2, expansion),  # 16->24 (s=2)\n",
        "            MV2Block(channels[2], channels[3], 1, expansion),  # 24->24\n",
        "            MV2Block(channels[3], channels[3], 1, expansion),  # 24->24\n",
        "            MV2Block(channels[3], channels[4], 2, expansion),  # 24->48 (s=2)\n",
        "            MV2Block(channels[4], channels[5], 1, expansion),  # 48->48\n",
        "            MV2Block(channels[5], channels[6], 2, expansion),  # 48->64 (s=2)\n",
        "        ])\n",
        "        self.mvit = nn.ModuleList([\n",
        "            MobileViTBlock(dims[0], depth=2, channel=channels[2], kernel_size=kernel_size, mlp_dim=dims[0]*2, dropout=dropout),\n",
        "            MobileViTBlock(dims[1], depth=4, channel=channels[4], kernel_size=kernel_size, mlp_dim=dims[1]*4, dropout=dropout),\n",
        "            MobileViTBlock(dims[2], depth=3, channel=channels[6], kernel_size=kernel_size, mlp_dim=dims[2]*4, dropout=dropout),\n",
        "        ])\n",
        "        self.fuse = conv_1x1_bn(channels[6], channels[7])\n",
        "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc   = nn.Linear(channels[7], num_classes)\n",
        "        self._init_weights()\n",
        "    def _init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None: nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.ones_(m.weight); nn.init.zeros_(m.bias)\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.mv2[0](x)\n",
        "        x = self.mv2[1](x); x = self.mv2[2](x); x = self.mvit[0](x)  # 24-ch\n",
        "        x = self.mv2[3](x); x = self.mv2[4](x); x = self.mv2[5](x); x = self.mvit[1](x)  # 48-ch\n",
        "        x = self.mv2[6](x); x = self.mvit[2](x)  # 64-ch\n",
        "        x = self.fuse(x)\n",
        "        x = self.pool(x).flatten(1)\n",
        "        return self.fc(x)\n",
        "\n",
        "class MobileViTClassifier(nn.Module):\n",
        "    def __init__(self, image_size=(224,224), num_classes=38, expansion=4, dropout=0.0):\n",
        "        super().__init__()\n",
        "        dims = [64, 80, 96]\n",
        "        channels = [16, 16, 24, 24, 48, 48, 64, 320]\n",
        "        self.model = MobileViT(image_size, dims, channels, num_classes, expansion=expansion, dropout=dropout)\n",
        "    def forward(self, x): return self.model(x)\n",
        "\n",
        "# sanity\n",
        "model = MobileViTClassifier(image_size=(IMG_SIZE, IMG_SIZE), num_classes=len(class_names)).to(device)\n",
        "print(\"Params (M):\", sum(p.numel() for p in model.parameters())/1e6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dcb5fcf",
      "metadata": {},
      "source": [
        "Cell C — Tiny MobileViT with patch tokens (8×8) + checkpointing option"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "99905c04",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from pathlib import Path\n",
        "\n",
        "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
        "\n",
        "def build_transforms(img_size=160, aug_level=\"light\"):\n",
        "    norm = transforms.Normalize(IMAGENET_MEAN, IMAGENET_STD)\n",
        "    if aug_level == \"light\":\n",
        "        train_tf = transforms.Compose([\n",
        "            transforms.Resize(int(img_size*1.1)),\n",
        "            transforms.RandomResizedCrop(img_size, scale=(0.9,1.0)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(), norm,\n",
        "        ])\n",
        "    else:\n",
        "        train_tf = transforms.Compose([\n",
        "            transforms.Resize(int(img_size*1.15)),\n",
        "            transforms.RandomResizedCrop(img_size, scale=(0.8,1.0)),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(), norm,\n",
        "        ])\n",
        "    val_tf = transforms.Compose([\n",
        "        transforms.Resize(int(img_size*1.1)),\n",
        "        transforms.CenterCrop(img_size),\n",
        "        transforms.ToTensor(), norm,\n",
        "    ])\n",
        "    return train_tf, val_tf\n",
        "\n",
        "def build_loaders(root: Path, img_size=160, batch_size=8, workers=0):\n",
        "    train_tf, val_tf = build_transforms(img_size, \"light\")\n",
        "    train_ds = datasets.ImageFolder(root/\"train\", transform=train_tf)\n",
        "    val_ds   = datasets.ImageFolder(root/\"test\",  transform=val_tf)\n",
        "    classes  = train_ds.classes\n",
        "    pin_mem  = torch.cuda.is_available()\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
        "                              num_workers=workers, pin_memory=pin_mem)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False,\n",
        "                              num_workers=workers, pin_memory=pin_mem)\n",
        "    return train_loader, val_loader, classes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "218d9705",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(38,\n",
              " ['Apple___Apple_scab',\n",
              "  'Apple___Black_rot',\n",
              "  'Apple___Cedar_apple_rust',\n",
              "  'Apple___healthy',\n",
              "  'Blueberry___healthy'])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os, torch\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "torch.cuda.empty_cache()\n",
        "torch.set_float32_matmul_precision(\"medium\")\n",
        "\n",
        "# ↓ shrink these to fit your GPU\n",
        "IMG_SIZE    = 160      # 160 < 192 < 224\n",
        "BATCH_SIZE  = 8        # try 4 or 2 if still OOM\n",
        "NUM_WORKERS = 0        # avoid RAM spikes from dataloader workers\n",
        "EPOCHS      = 3\n",
        "\n",
        "train_loader, val_loader, class_names = build_loaders(TRAIN_ROOT, IMG_SIZE, BATCH_SIZE, NUM_WORKERS)\n",
        "len(class_names), class_names[:5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "3eb28c52",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time, math\n",
        "from tqdm import tqdm\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, optimizer, criterion, scheduler, device, out_dir=\"./outputs\"):\n",
        "        from pathlib import Path\n",
        "        self.model, self.opt, self.crit, self.sched = model, optimizer, criterion, scheduler\n",
        "        self.device = device\n",
        "        self.out_dir = Path(out_dir); (self.out_dir/\"checkpoints\").mkdir(parents=True, exist_ok=True)\n",
        "        self.scaler = torch.amp.GradScaler('cuda', enabled=(device.type == \"cuda\"))\n",
        "\n",
        "    def _epoch(self, loader, train=True):\n",
        "        self.model.train(mode=train)\n",
        "        tot_loss, correct, total = 0.0, 0, 0\n",
        "        it = tqdm(loader, leave=False)\n",
        "        for x, y in it:\n",
        "            # channels_last on inputs too\n",
        "            x = x.to(self.device, non_blocking=True).to(memory_format=torch.channels_last)\n",
        "            y = y.to(self.device, non_blocking=True)\n",
        "\n",
        "            with torch.amp.autocast('cuda', enabled=(self.device.type==\"cuda\")):\n",
        "                logits = self.model(x)\n",
        "                loss = self.crit(logits, y)\n",
        "\n",
        "            if train:\n",
        "                self.opt.zero_grad(set_to_none=True)\n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.step(self.opt)\n",
        "                self.scaler.update()\n",
        "\n",
        "            bs = x.size(0)\n",
        "            tot_loss += loss.item() * bs\n",
        "            correct  += (logits.argmax(1) == y).sum().item()\n",
        "            total    += bs\n",
        "            it.set_postfix(loss=f\"{loss.item():.3f}\", acc=f\"{correct/max(1,total):.3f}\")\n",
        "        return tot_loss/max(1,total), correct/max(1,total)\n",
        "\n",
        "    def fit(self, train_loader, val_loader, epochs=3):\n",
        "        best = math.inf\n",
        "        for e in range(1, epochs+1):\n",
        "            t0 = time.time()\n",
        "            tr_loss, tr_acc = self._epoch(train_loader, True)\n",
        "            with torch.no_grad():\n",
        "                va_loss, va_acc = self._epoch(val_loader, False)\n",
        "            if self.sched is not None:\n",
        "                try: self.sched.step(va_loss)\n",
        "                except: self.sched.step()\n",
        "            if va_loss < best:\n",
        "                best = va_loss\n",
        "                torch.save({\"model\": self.model.state_dict()}, self.out_dir/\"checkpoints\"/\"best.pth\")\n",
        "            print(f\"Epoch {e:02d} | train {tr_loss:.4f}/{tr_acc:.3f} | val {va_loss:.4f}/{va_acc:.3f} | {time.time()-t0:.1f}s\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "7d1c7eea",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/11203 [00:00<?, ?it/s]/home/ali/ws/xaje-nasir/mechatronic/project/python-mech1/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "                                                                          \r"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mCosineAnnealingLR(optimizer, T_max\u001b[38;5;241m=\u001b[39mEPOCHS)\n\u001b[1;32m      5\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, optimizer, criterion, scheduler, device, out_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[19], line 42\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, train_loader, val_loader, epochs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     41\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 42\u001b[0m     tr_loss, tr_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     44\u001b[0m         va_loss, va_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epoch(val_loader, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "Cell \u001b[0;32mIn[19], line 27\u001b[0m, in \u001b[0;36mTrainer._epoch\u001b[0;34m(self, loader, train)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopt)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mupdate()\n",
            "File \u001b[0;32m~/ws/xaje-nasir/mechatronic/project/python-mech1/lib/python3.12/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/ws/xaje-nasir/mechatronic/project/python-mech1/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/ws/xaje-nasir/mechatronic/project/python-mech1/lib/python3.12/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "trainer = Trainer(model, optimizer, criterion, scheduler, device, out_dir=\"./outputs\")\n",
        "trainer.fit(train_loader, val_loader, epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7979436d",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# load best\n",
        "ckpt = torch.load(\"./outputs/checkpoints/best.pth\", map_location=device)\n",
        "model.load_state_dict(ckpt.get(\"model\", ckpt))\n",
        "model.eval()\n",
        "\n",
        "y_true, y_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for x, y in val_loader:\n",
        "        x = x.to(device)\n",
        "        logits = model(x)\n",
        "        y_pred.extend(logits.argmax(1).cpu().tolist())\n",
        "        y_true.extend(y.tolist())\n",
        "\n",
        "acc = (torch.tensor(y_true) == torch.tensor(y_pred)).float().mean().item()\n",
        "print(\"Val Accuracy:\", f\"{acc:.4f}\")\n",
        "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "python-mech1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
